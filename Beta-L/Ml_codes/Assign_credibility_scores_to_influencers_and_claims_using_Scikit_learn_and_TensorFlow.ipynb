{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGnu0h9yf9T0",
        "outputId": "456cb1e0-31d3-41e2-d3a0-4501a440d68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    If ticker of Indian Stock Market, add \".NS\" at last\n",
            "    For example -- \"ADANIPOWER.NS\" for Adani Power\n",
            "                -- \"TATAMOTORS.NS\" for Tata Motors\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_datareader as web\n",
        "import datetime as dt\n",
        "from sklearn.manifold import trustworthiness\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, LSTM\n",
        "\n",
        "# Load Data\n",
        "print(\"\"\"\n",
        "    If ticker of Indian Stock Market, add \".NS\" at last\n",
        "    For example -- \"ADANIPOWER.NS\" for Adani Power\n",
        "                -- \"TATAMOTORS.NS\" for Tata Motors\n",
        "\"\"\")\n",
        "\n",
        "company = input(\"Enter ticker symbol :\" ).upper()\n",
        "\n",
        "start = dt.datetime(2012,1,1)\n",
        "end = dt.datetime(2022,1,1)\n",
        "\n",
        "data = web.DataReader(company, 'yahoo', start, end)\n",
        "# Prepare Data\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1,1))\n",
        "prediction_days = 60\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "print(\"Collecting data...\")\n",
        "\n",
        "for x in range(prediction_days, len(scaled_data)):\n",
        "    x_train.append(scaled_data[x-prediction_days:x,0])\n",
        "    y_train.append(scaled_data[x,0])\n",
        "\n",
        "# Converting to numpy arrays\n",
        "\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
        "\n",
        "print(\"Initializing...\")\n",
        "\n",
        "# Training Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1)) # Prediction for next price\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# Testing\n",
        "\n",
        "test_start = dt.datetime(2020,1,1)\n",
        "test_end = dt.datetime(2022,1,1)\n",
        "\n",
        "test_data = web.DataReader(company, 'yahoo', test_start, test_end)\n",
        "actual_price = test_data['Close'].values\n",
        "\n",
        "total_dataset = pd.concat((data['Close'], test_data['Close']), axis=0)\n",
        "\n",
        "model_inputs = total_dataset[len(total_dataset) - len(test_data) - prediction_days:].values\n",
        "model_inputs = model_inputs.reshape(-1,1)\n",
        "model_inputs = scaler.transform(model_inputs)\n",
        "\n",
        "# Prediction\n",
        "\n",
        "x_test = []\n",
        "\n",
        "print(\"Predicting...\")\n",
        "\n",
        "for x in range(prediction_days, len(model_inputs)):\n",
        "    x_test.append(model_inputs[x-prediction_days:x,0])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "predicted_prices = model.predict(x_test)\n",
        "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
        "\n",
        "# Prediction for next day\n",
        "\n",
        "\n",
        "real_data = [model_inputs[len(model_inputs) + 1 - prediction_days:len(model_inputs+1), 0]]\n",
        "real_data = np.array(real_data)\n",
        "real_data = np.reshape(real_data,(real_data.shape[0],real_data.shape[1],1))\n",
        "\n",
        "prediction = model.predict(real_data)\n",
        "prediction = scaler.inverse_transform(prediction)\n",
        "print(f\"Prediction {prediction} \")\n",
        "\n",
        "# Ploting the prediction\n",
        "\n",
        "plt.plot(actual_price, color=\"blue\", label=f\"Actual {company} price...\")\n",
        "plt.plot(predicted_prices, color=\"green\", label=f\"Predicted {company} price...\")\n",
        "plt.title(f'{company} share prices...')\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(f\"{company} share price\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wq-ZJRaacpk"
      },
      "source": [
        "supervised algorithm for assigning credibility scores to influencers and claims using Scikit-learn and TensorFlow in Indian stock markets using machine learning and deep learning techniques:\n",
        "\n",
        "1)Collect a dataset of historical data on stock prices, influencers, and claims. This data can be scraped from social media platforms, news websites, and other sources.\n",
        "2)Preprocess the data by cleaning it and removing any noise.\n",
        "3)Feature engineer the data by extracting the relevant features. The features that you can consider include:\n",
        "Whether the influencer is registered as an advisor with SEBI\n",
        "Whether they follow SEBIâ€™s registered advisor guidelines\n",
        "Whether they have any action pending from SEBI\n",
        "Whether they disclose any commission or conflict of interest\n",
        "Whether they provide evidence or references for their claim\n",
        "Whether they have a consistent track record of performance\n",
        "Whether they have a large and engaged following\n",
        "4)Train a supervised learning model on the preprocessed data. You can use a model such as logistic regression, support vector machines, or random forests.\n",
        "5)Evaluate the performance of the model on a held-out test set.\n",
        "6)Use the model to assign credibility scores to new influencers and claims.\n",
        "\n",
        "Here are some specific algorithms that you could use:\n",
        "\n",
        "Logistic regression: This is a simple but effective algorithm that can be used for binary classification tasks. In this case, the binary classification task would be to classify whether an influencer or claim is credible or not.\n",
        "Support vector machine: This is a more powerful algorithm that can be used for both binary and multi-class classification tasks. It is generally more accurate than logistic regression, but it can also be more computationally expensive.\n",
        "Random forest: This is an ensemble learning algorithm that combines the predictions of multiple decision trees. It is often more accurate than logistic regression or support vector machines, but it can also be more computationally expensive.\n",
        "The specific algorithm that you choose will depend on the size and complexity of your dataset, as well as the desired accuracy of the model.\n",
        "\n",
        "Scikit-learn is a popular Python library for machine learning. It provides implementations of many supervised learning algorithms, including logistic regression, support vector machines, and random forests. TensorFlow is another popular Python library for machine learning. It is a more general-purpose library that can be used for both supervised and unsupervised learning tasks.\n",
        "\n",
        "The following are some of the challenges that you might face when developing a supervised algorithm for assigning credibility scores to influencers and claims in Indian stock markets:\n",
        "\n",
        "The data is often noisy and incomplete.\n",
        "The data is often biased.\n",
        "The data is constantly changing.\n",
        "It can be difficult to define what constitutes a credible influencer or claim.\n",
        "Despite these challenges, it is possible to develop a supervised algorithm for assigning credibility scores to influencers and claims in Indian stock markets. By carefully cleaning and feature engineering the data, and by using a supervised learning model, you can develop a model that can be used to assign credibility scores to new influencers and claims with a high degree of accuracy.\n",
        "\n",
        "Here are some additional considerations for developing a supervised algorithm for assigning credibility scores to influencers and claims in Indian stock markets:\n",
        "\n",
        "The algorithm should be transparent and accountable. Users should be able to understand how the algorithm works and why it assigns the credibility scores that it does.\n",
        "The algorithm should be fair and unbiased. It should not discriminate against any particular group of influencers or claims.\n",
        "The algorithm should be robust to changes in the data. It should continue to work accurately even if the data changes over time."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}